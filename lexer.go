package langtools

import "unicode/utf8"

type ILexer interface {
	Errors() []LexerError
	Next() (Token, bool)
}

type Lexer struct {
	MaxErrors int

	input       []byte
	tokenizerFn func(*Lexer) Token

	nextTokens []Token
	nextChars  []Char
	errors     []LexerError
	cursor     int
	line       int
	column     int
	eof        *Token
}

// Creates a new lexer with the given input and tokenizerFn.
//
// The tokenizerFn is a function that receives a lexer and returns the next
// token from the input. Use the functions `EatChar`, `PeekChar` and
// `PeekCharAt` to read the input accordingly. If the lexer has reached the
// end of the input, it will return an empty char upon reading. The last token
// generated by this function will be the EOF token, and will be returned
// continuously after the end of the input is reached.
//
// Example:
//
// 		lexer := NewLexer(input, func(l *Lexer) Token {
// 				c := l.EatChar()
// 				if isDigit(c.Rune) {
// 					return NewToken(NUMBER, c)
// 				}
// 				return NewToken(UNKNOWN, c)
// 		})
//
func NewLexer(input []byte, tokenizerFn func(*Lexer) Token) *Lexer {
	return &Lexer{
		MaxErrors:   10,
		input:       input,
		tokenizerFn: tokenizerFn,
		nextTokens:  make([]Token, 0),
		nextChars:   make([]Char, 0),
		errors:      make([]LexerError, 0),
		cursor:      0,
		line:        1,
		column:      1,
		eof:         nil,
	}
}

// Registers a new error found by the lexer. If the number of errors is greater
// than the maximum allowed, the error is ignored.
func (l *Lexer) RegisterError(msg string) {
	if len(l.errors) >= l.MaxErrors {
		return
	}

	l.errors = append(l.errors, NewLexerError(l.line, l.column, msg))
}

// Returns the errors found by the lexer.
func (l *Lexer) Errors() []LexerError {
	return l.errors
}

// Returns true if the lexer has errors.
func (l *Lexer) HasErrors() bool {
	return len(l.errors) > 0
}

// Returns true if the lexer has too many errors.
func (l *Lexer) HasTooManyErrors() bool {
	return len(l.errors) >= l.MaxErrors
}

// Returns true if the cursor is at the end of the input.
func (l *Lexer) IsEof() bool {
	return l.cursor >= len(l.input) || l.HasTooManyErrors()
}

// Range func
//
// Example:
//
// 		for i, t := range l.Iter() {
// 			// Do something with i and t
// 		}
func (l *Lexer) Iter() func(func(int, Token) bool) {
	return func(f func(int, Token) bool) {
		i := -1
		for {
			i++
			t := l.EatToken()
			if !f(i, t) {
				return
			}

			if len(l.nextTokens) <= 0 && l.IsEof() {
				return
			}
		}
	}
}

// Helper to iterate over the tokens of the lexer.
//
// Example:
//
// 		for {
// 			t, eof := l.Next()
// 			if eof {
// 				break
// 			}
//
// 			// Do something with t
// 		}
//
func (l *Lexer) Next() (token Token, eof bool) {
	return l.EatToken(), l.IsEof()
}

// Reads the next token from input and consumes it, moving the cursor forward.
// If the cursor is at the end of the input, it always returns the last valid
// token.
func (l *Lexer) EatToken() Token {
	t := l.PeekToken()
	if len(l.nextTokens) > 0 {
		l.nextTokens = l.nextTokens[1:]
	}

	return t
}

// Reads the current token from the input. The cursor is not moved, so it can be
// called multiple times without consuming the input.
func (l *Lexer) PeekToken() Token {
	return l.PeekTokenAt(0)
}

// Reads the token at the given offset. The offset starts at 0, meaning the
// current token. Peek does not move the cursor, so it can be called multiple
// times without consuming the input.
func (l *Lexer) PeekTokenAt(offset int) Token {
	for len(l.nextTokens) <= offset {
		if l.eof != nil {
			return *l.eof
		}

		if l.IsEof() {
			t := NewToken(TEof, "", l.line, l.column)
			l.eof = &t
			return *l.eof
		}

		t := l.tokenizerFn(l)
		if l.HasTooManyErrors() {
			l.eof = &t
		}

		l.nextTokens = append(l.nextTokens, t)
	}

	return l.nextTokens[offset]
}

// Reads the next character from input and consumes it, moving the cursor
// forward. If the cursor is at the end of the input, it returns an empty char.
func (l *Lexer) EatChar() Char {
	c := l.PeekChar()
	l.nextChars = l.nextChars[1:]
	return c
}

// Reads the current character from the input. The cursor is not moved, so it
// can be called multiple times without consuming the input.
func (l *Lexer) PeekChar() Char {
	return l.PeekCharAt(0)
}

// Reads the character at the given offset. The offset starts a 0, meaning the
// current character. Peek does not move the cursor, so it can be called
// multiple times without consuming the input.
func (l *Lexer) PeekCharAt(offset int) Char {
	for len(l.nextChars) <= offset {
		l.nextChars = append(l.nextChars, l.nextChar())
	}

	return l.nextChars[offset]
}

// Returns the next valid token from the input. If the input is empty, or the
// lexer has too many errors, or the file ended it returns an empty char.
func (l *Lexer) nextChar() Char {
	for l.cursor < len(l.input) {
		if l.HasTooManyErrors() {
			return NewChar(l.line, l.column, 0, 0)
		}

		r, size := utf8.DecodeRune(l.input[l.cursor:])
		if r == utf8.RuneError {
			l.RegisterError(ErrInvalidChar)
			l.cursor += size
			continue
		}

		c := NewChar(l.line, l.column, size, r)
		l.cursor += size
		l.column++
		if c.Is('\n') {
			l.line++
			l.column = 1
		}

		return c
	}

	return NewChar(l.line, l.column, 0, 0)
}
