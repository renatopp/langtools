package lexers

import (
	"github.com/renatopp/langtools/token"
)

type ILexer interface {
	Errors() []LexerError
	Next() (token.Token, bool)
}

type Lexer struct {
	MaxErrors int

	scanner     *Scanner
	tokenizerFn func(*Lexer) token.Token

	nextTokens []token.Token
	nextChars  []token.Char
	errors     []LexerError
	eof        *token.Token
}

// Creates a new lexer with the given input and tokenizerFn.
//
// The tokenizerFn is a function that receives a lexer and returns the next
// token from the input. Use the functions `EatChar`, `PeekChar` and
// `PeekCharAt` to read the input accordingly. If the lexer has reached the
// end of the input, it will return an empty char upon reading. The last token
// generated by this function will be the EOF token, and will be returned
// continuously after the end of the input is reached.
//
// Example:
//
//	lexer := NewLexer(input, func(l *Lexer) Token {
//			c := l.EatChar()
//			if isDigit(c.Rune) {
//				return NewToken(NUMBER, c)
//			}
//			return NewToken(UNKNOWN, c)
//	})
func NewLexer(input []byte, tokenizerFn func(*Lexer) token.Token) *Lexer {
	return &Lexer{
		MaxErrors:   10,
		scanner:     NewScanner(input),
		tokenizerFn: tokenizerFn,
		nextTokens:  make([]token.Token, 0),
		nextChars:   make([]token.Char, 0),
		errors:      make([]LexerError, 0),
		eof:         nil,
	}
}

// Registers a new error found by the lexer. If the number of errors is greater
// than the maximum allowed, the error is ignored.
func (l *Lexer) RegisterError(msg string) {
	if len(l.errors) >= l.MaxErrors {
		return
	}

	l.errors = append(l.errors, NewLexerError(l.scanner.line, l.scanner.column, msg))
}

// Returns the errors found by the lexer.
func (l *Lexer) Errors() []LexerError {
	return l.errors
}

// Returns true if the lexer has errors.
func (l *Lexer) HasErrors() bool {
	return len(l.errors) > 0
}

// Returns true if the lexer has too many errors.
func (l *Lexer) HasTooManyErrors() bool {
	return len(l.errors) >= l.MaxErrors
}

// Returns true if the cursor is at the end of the input.
func (l *Lexer) IsEof() bool {
	return l.PeekChar().Rune == 0 || l.HasTooManyErrors()
}

// Range func
//
// Example:
//
//	for i, t := range l.Iter() {
//		// Do something with i and t
//	}
func (l *Lexer) Iter() func(func(int, token.Token) bool) {
	return func(f func(int, token.Token) bool) {
		i := -1
		for {
			i++
			t := l.EatToken()
			if !f(i, t) {
				return
			}

			if len(l.nextTokens) <= 0 && l.IsEof() {
				return
			}
		}
	}
}

// Helper to iterate over the tokens of the lexer.
//
// Example:
//
//	for {
//		t, eof := l.Next()
//		if eof {
//			break
//		}
//
//		// Do something with t
//	}
func (l *Lexer) Next() (token token.Token, eof bool) {
	a, b := l.EatToken(), l.IsEof()
	return a, b
}

// Reads the next token from input and consumes it, moving the cursor forward.
// If the cursor is at the end of the input, it always returns the last valid
// token.
func (l *Lexer) EatToken() token.Token {
	t := l.PeekToken()
	if len(l.nextTokens) > 0 {
		l.nextTokens = l.nextTokens[1:]
	}

	return t
}

// Reads the current token from the input. The cursor is not moved, so it can be
// called multiple times without consuming the input.
func (l *Lexer) PeekToken() token.Token {
	return l.PeekTokenAt(0)
}

// Reads the token at the given offset. The offset starts at 0, meaning the
// current token. Peek does not move the cursor, so it can be called multiple
// times without consuming the input.
func (l *Lexer) PeekTokenAt(offset int) token.Token {
	for len(l.nextTokens) <= offset {
		if l.eof != nil {
			return *l.eof
		}

		t := l.tokenizerFn(l)
		if l.HasTooManyErrors() || t.Type == token.TEof {
			l.eof = &t
		}

		l.nextTokens = append(l.nextTokens, t)
	}

	return l.nextTokens[offset]
}

// Reads the next character from input and consumes it, moving the cursor
// forward. If the cursor is at the end of the input, it returns an empty char.
func (l *Lexer) EatChar() token.Char {
	c := l.PeekChar()
	if len(l.nextChars) > 0 {
		l.nextChars = l.nextChars[1:]
	}
	return c
}

// Reads the current character from the input. The cursor is not moved, so it
// can be called multiple times without consuming the input.
func (l *Lexer) PeekChar() token.Char {
	return l.PeekCharAt(0)
}

// Reads the character at the given offset. The offset starts a 0, meaning the
// current character. Peek does not move the cursor, so it can be called
// multiple times without consuming the input.
func (l *Lexer) PeekCharAt(offset int) token.Char {
	for len(l.nextChars) <= offset {
		if l.HasTooManyErrors() {
			return token.Char{Rune: 0, Size: 1, Line: 1, Column: 1}
		}

		char, err := l.scanner.Next()
		if err != nil {
			l.RegisterError(err.Error())
		}
		l.nextChars = append(l.nextChars, char)
	}

	return l.nextChars[offset]
}
